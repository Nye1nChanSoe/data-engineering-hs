{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c072bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    DateType,\n",
    "    BooleanType,\n",
    "    TimestampType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    to_timestamp,\n",
    "    to_date,\n",
    "    when,\n",
    "    broadcast,\n",
    "    count,\n",
    "    sum as spark_sum,\n",
    "    count_distinct,\n",
    ")\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbdb7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Spark Context Info =====\n",
      "App Name      : Revenue Analysis\n",
      "Master        : local[8]\n",
      "Application ID: local-1758562105877\n",
      "UI Web URL    : http://172.20.10.14:4040\n",
      "Version       : 4.0.1\n",
      "Python Ver    : 3.11\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Revenue Analysis\")\n",
    "    .master(\"local[8]\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    # .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://127.0.0.1:9000\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark_context = spark.sparkContext.getOrCreate()\n",
    "\n",
    "print(\"\\n===== Spark Context Info =====\")\n",
    "print(f\"App Name      : {spark_context.appName}\")\n",
    "print(f\"Master        : {spark_context.master}\")\n",
    "print(f\"Application ID: {spark_context.applicationId}\")\n",
    "print(f\"UI Web URL    : {spark_context.uiWebUrl}\")\n",
    "print(f\"Version       : {spark_context.version}\")\n",
    "print(f\"Python Ver    : {spark_context.pythonVer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83febe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[0;32m\"item_id\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"category\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"tags\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n",
      "\u001b[1;39m[\n",
      "  \u001b[0;32m\"id\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"signup_date\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"plan\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"country\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"marketing_opt_in\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/items.jsonl | jq \"keys_unsorted\" \n",
    "!head -n 1 data/users.jsonl | jq \"keys_unsorted\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96050061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[1;34m\"ts\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2025-08-23T13:43:20.968573+00:00\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"event\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"view\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"user_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m12482\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"item_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3686\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"country\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"US\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"device\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"android\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"locale\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"pt_BR\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"session_id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"bdd640fb-0667-4ad1-9c80-317fa3b1799d\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"props\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"price\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"payment_method\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[1;34m\"dwell_ms\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1232\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[1;34m\"exp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[1;34m\"ab_group\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"B\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n",
      "gunzip: error writing to output: Broken pipe\n",
      "gunzip: data/events/part-00.jsonl.gz: uncompress failed\n"
     ]
    }
   ],
   "source": [
    "!gunzip -c data/events/part-00.jsonl.gz | head -n 1 | jq \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81aa1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = StructType(\n",
    "    [\n",
    "        StructField(\"item_id\", IntegerType(), False),\n",
    "        StructField(\"category\", StringType(), False),\n",
    "        StructField(\"tags\", ArrayType(StringType()), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "user_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"signup_date\", DateType(), False),\n",
    "        StructField(\"plan\", StringType(), False),\n",
    "        StructField(\"country\", StringType(), False),\n",
    "        StructField(\"marketing_opt_in\", BooleanType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "event_schema = StructType(\n",
    "    [\n",
    "        StructField(\"ts\", TimestampType(), False),\n",
    "        StructField(\"event\", StringType(), False),\n",
    "        StructField(\"user_id\", IntegerType(), False),\n",
    "        StructField(\"item_id\", IntegerType(), False),\n",
    "        StructField(\n",
    "            \"context\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"country\", StringType(), False),\n",
    "                    StructField(\"device\", StringType(), False),\n",
    "                    StructField(\"locale\", StringType(), False),\n",
    "                    StructField(\"session_id\", StringType(), False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\n",
    "            \"props\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"price\", DoubleType(), True),\n",
    "                    StructField(\"payment_method\", StringType(), True),\n",
    "                    StructField(\"dwell_ms\", IntegerType(), True),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        StructField(\n",
    "            \"exp\",\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"ab_group\", StringType(), True),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bfe0972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users | total - 20000 | partitions - 1\n",
      "+---+-----------+----+-------+----------------+\n",
      "| id|signup_date|plan|country|marketing_opt_in|\n",
      "+---+-----------+----+-------+----------------+\n",
      "|  1| 2024-09-29|free|     TH|           false|\n",
      "|  2| 2025-04-30|free|     DE|            true|\n",
      "|  3| 2024-08-08| pro|     DE|            true|\n",
      "|  4| 2025-01-18|free|     DE|           false|\n",
      "|  5| 2025-04-25|free|     US|            true|\n",
      "+---+-----------+----+-------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "items | total - 5000 | partitions - 1\n",
      "+-------+-----------+--------------------+\n",
      "|item_id|   category|                tags|\n",
      "+-------+-----------+--------------------+\n",
      "|      1|     sports|    [new, clearance]|\n",
      "|      2|     sports|[sale, popular, c...|\n",
      "|      3|     sports|   [new, gift, sale]|\n",
      "|      4|      books|              [gift]|\n",
      "|      5|electronics|     [sale, popular]|\n",
      "+-------+-----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_users = spark.read.schema(user_schema).json(\"data/users.jsonl\")\n",
    "df_items = spark.read.schema(items_schema).json(\"data/items.jsonl\")\n",
    "df_events = spark.read.schema(event_schema).json(\n",
    "    [\n",
    "        \"data/events/part-00.jsonl.gz\",\n",
    "        \"data/events/part-01.jsonl.gz\",\n",
    "        \"data/events/part-02.jsonl.gz\",\n",
    "        \"data/events/part-03.jsonl.gz\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"users | total - {df_users.count()} | partitions - {df_users.rdd.getNumPartitions()}\"\n",
    ")\n",
    "df_users.show(5)\n",
    "print(\n",
    "    f\"\\nitems | total - {df_items.count()} | partitions - {df_items.rdd.getNumPartitions()}\"\n",
    ")\n",
    "df_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba2de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events | total - 200000 | partitions - 4\n",
      "+--------------------+-----+-------+-------+--------------------+------------------+---+\n",
      "|                  ts|event|user_id|item_id|             context|             props|exp|\n",
      "+--------------------+-----+-------+-------+--------------------+------------------+---+\n",
      "|2025-08-23 20:43:...| view|  12482|   3686|{US, android, pt_...|{NULL, NULL, 1232}|{B}|\n",
      "|2025-08-23 20:43:...| view|   2361|   3322|{GB, ios, vi_VN, ...|{NULL, NULL, 4895}|{A}|\n",
      "|2025-08-23 20:43:...| view|  17249|   3108|{US, web, fr_FR, ...|{NULL, NULL, 3663}|{A}|\n",
      "|2025-08-23 20:43:...| view|  16719|   2772|{US, web, th_TH, ...|{NULL, NULL, 4178}|{A}|\n",
      "|2025-08-23 20:45:...| view|   1937|     69|{IN, web, fr_FR, ...|{NULL, NULL, 2148}|{A}|\n",
      "+--------------------+-----+-------+-------+--------------------+------------------+---+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_events = spark.read.schema(event_schema).json(\n",
    "    [\n",
    "        \"data/events/part-00.jsonl.gz\",\n",
    "        \"data/events/part-01.jsonl.gz\",\n",
    "        \"data/events/part-02.jsonl.gz\",\n",
    "        \"data/events/part-03.jsonl.gz\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"events | total - {df_events.count()} | partitions - {df_events.rdd.getNumPartitions()}\"\n",
    ")\n",
    "df_events.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3ff7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = df_events.withColumn(\"timestamp\", to_timestamp(\"ts\")).withColumn(\n",
    "    \"date\", to_date(\"ts\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2442ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events | total - 199995 | partitions - 4\n"
     ]
    }
   ],
   "source": [
    "df_events = (\n",
    "    df_events.withColumn(\n",
    "        \"revenue\",\n",
    "        when(col(\"event\") == \"purchase\", col(\"props.price\").cast(DoubleType()))\n",
    "        .otherwise(0.0)\n",
    "        .cast(DoubleType()),\n",
    "    )\n",
    ").filter(col(\"revenue\") >= 0.0)\n",
    "\n",
    "print(\n",
    "    f\"events | total - {df_events.count()} | partitions - {df_events.rdd.getNumPartitions()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8431ea",
   "metadata": {},
   "source": [
    "#### broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bd8916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join LeftOuter, (user_id#527 = id#450)\n",
      ":- Project [item_id#528, ts#525, event#526, user_id#527, context#529, props#530, exp#531, timestamp#580, date#581, revenue#582, category#456, tags#457]\n",
      ":  +- Join LeftOuter, (item_id#528 = item_id#455)\n",
      ":     :- Filter (revenue#582 >= 0.0)\n",
      ":     :  +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, timestamp#580, date#581, cast(CASE WHEN (event#526 = purchase) THEN cast(props#530.price as double) ELSE 0.0 END as double) AS revenue#582]\n",
      ":     :     +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, timestamp#580, to_date(ts#525, None, Some(Asia/Bangkok), true) AS date#581]\n",
      ":     :        +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, to_timestamp(ts#525, None, TimestampType, Some(Asia/Bangkok), true) AS timestamp#580]\n",
      ":     :           +- Relation [ts#525,event#526,user_id#527,item_id#528,context#529,props#530,exp#531] json\n",
      ":     +- ResolvedHint (strategy=broadcast)\n",
      ":        +- Relation [item_id#455,category#456,tags#457] json\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [id#450,signup_date#451,plan#452,country#453,marketing_opt_in#454] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "item_id: int, ts: timestamp, event: string, user_id: int, context: struct<country:string,device:string,locale:string,session_id:string>, props: struct<price:double,payment_method:string,dwell_ms:int>, exp: struct<ab_group:string>, timestamp: timestamp, date: date, revenue: double, category: string, tags: array<string>, id: int, signup_date: date, plan: string, country: string, marketing_opt_in: boolean\n",
      "Join LeftOuter, (user_id#527 = id#450)\n",
      ":- Project [item_id#528, ts#525, event#526, user_id#527, context#529, props#530, exp#531, timestamp#580, date#581, revenue#582, category#456, tags#457]\n",
      ":  +- Join LeftOuter, (item_id#528 = item_id#455)\n",
      ":     :- Filter (revenue#582 >= 0.0)\n",
      ":     :  +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, timestamp#580, date#581, cast(CASE WHEN (event#526 = purchase) THEN cast(props#530.price as double) ELSE 0.0 END as double) AS revenue#582]\n",
      ":     :     +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, timestamp#580, to_date(ts#525, None, Some(Asia/Bangkok), true) AS date#581]\n",
      ":     :        +- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, to_timestamp(ts#525, None, TimestampType, Some(Asia/Bangkok), true) AS timestamp#580]\n",
      ":     :           +- Relation [ts#525,event#526,user_id#527,item_id#528,context#529,props#530,exp#531] json\n",
      ":     +- ResolvedHint (strategy=broadcast)\n",
      ":        +- Relation [item_id#455,category#456,tags#457] json\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [id#450,signup_date#451,plan#452,country#453,marketing_opt_in#454] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join LeftOuter, (user_id#527 = id#450), rightHint=(strategy=broadcast)\n",
      ":- Project [item_id#528, ts#525, event#526, user_id#527, context#529, props#530, exp#531, timestamp#580, date#581, revenue#582, category#456, tags#457]\n",
      ":  +- Join LeftOuter, (item_id#528 = item_id#455), rightHint=(strategy=broadcast)\n",
      ":     :- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, ts#525 AS timestamp#580, cast(ts#525 as date) AS date#581, CASE WHEN (event#526 = purchase) THEN props#530.price ELSE 0.0 END AS revenue#582]\n",
      ":     :  +- Filter CASE WHEN (event#526 = purchase) THEN (props#530.price >= 0.0) ELSE true END\n",
      ":     :     +- Relation [ts#525,event#526,user_id#527,item_id#528,context#529,props#530,exp#531] json\n",
      ":     +- Filter isnotnull(item_id#455)\n",
      ":        +- Relation [item_id#455,category#456,tags#457] json\n",
      "+- Filter isnotnull(id#450)\n",
      "   +- Relation [id#450,signup_date#451,plan#452,country#453,marketing_opt_in#454] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [user_id#527], [id#450], LeftOuter, BuildRight, false\n",
      "   :- Project [item_id#528, ts#525, event#526, user_id#527, context#529, props#530, exp#531, timestamp#580, date#581, revenue#582, category#456, tags#457]\n",
      "   :  +- BroadcastHashJoin [item_id#528], [item_id#455], LeftOuter, BuildRight, false\n",
      "   :     :- Project [ts#525, event#526, user_id#527, item_id#528, context#529, props#530, exp#531, ts#525 AS timestamp#580, cast(ts#525 as date) AS date#581, CASE WHEN (event#526 = purchase) THEN props#530.price ELSE 0.0 END AS revenue#582]\n",
      "   :     :  +- Filter CASE WHEN (event#526 = purchase) THEN (props#530.price >= 0.0) ELSE true END\n",
      "   :     :     +- FileScan json [ts#525,event#526,user_id#527,item_id#528,context#529,props#530,exp#531] Batched: false, DataFilters: [CASE WHEN (event#526 = purchase) THEN (props#530.price >= 0.0) ELSE true END], Format: JSON, Location: InMemoryFileIndex(4 paths)[file:/Users/nyeinchan/Desktop/DE_lab/revenue_analysis/data/events/part..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ts:timestamp,event:string,user_id:int,item_id:int,context:struct<country:string,device:str...\n",
      "   :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2011]\n",
      "   :        +- Filter isnotnull(item_id#455)\n",
      "   :           +- FileScan json [item_id#455,category#456,tags#457] Batched: false, DataFilters: [isnotnull(item_id#455)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/nyeinchan/Desktop/DE_lab/revenue_analysis/data/items.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(item_id)], ReadSchema: struct<item_id:int,category:string,tags:array<string>>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2015]\n",
      "      +- Filter isnotnull(id#450)\n",
      "         +- FileScan json [id#450,signup_date#451,plan#452,country#453,marketing_opt_in#454] Batched: false, DataFilters: [isnotnull(id#450)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/nyeinchan/Desktop/DE_lab/revenue_analysis/data/users.jsonl], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,signup_date:date,plan:string,country:string,marketing_opt_in:boolean>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark shuffles both side of joins\n",
    "# shuffles -> computationally expension and i/o operations included\n",
    "# broadcast -> ships small tables to every executors\n",
    "# now executors can join locally with their respective partitions\n",
    "\n",
    "df_joined = df_events.join(broadcast(df_items), on=\"item_id\", how=\"left\").join(\n",
    "    broadcast(df_users), df_events.user_id == df_users.id, how=\"left\"\n",
    ")\n",
    "\n",
    "df_joined.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa3b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregations | total - 3368 | partitions - 1\n",
      "+----------+-------+--------+------------+-------------+------------------+------------+\n",
      "|      date|country|category|total_events|num_purchases|     total_revenue|unique_users|\n",
      "+----------+-------+--------+------------+-------------+------------------+------------+\n",
      "|2025-08-26|     DE|   books|         190|            7|1194.5700000000002|         180|\n",
      "|2025-09-02|     TH|   books|         123|            1|             93.62|         120|\n",
      "|2025-09-10|     DE|    toys|         105|            2|            261.39|         102|\n",
      "|2025-09-04|     VN|    toys|          97|            6|            792.43|          97|\n",
      "|2025-09-11|     FR|   books|          84|            2|            103.65|          82|\n",
      "+----------+-------+--------+------------+-------------+------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_aggregate = df_joined.groupBy(\"date\", \"country\", \"category\").agg(\n",
    "    count(\"*\").alias(\"total_events\"),\n",
    "    count(when(col(\"event\") == \"purchase\", 1)).alias(\"num_purchases\"),\n",
    "    spark_sum(\"revenue\").alias(\"total_revenue\"),\n",
    "    count_distinct(\"user_id\").alias(\"unique_users\"),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"aggregations | total - {df_aggregate.count()} | partitions - {df_aggregate.rdd.getNumPartitions()}\"\n",
    ")\n",
    "df_aggregate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172543f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"country\", \"category\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "df_final = df_aggregate.withColumn(\"revenue_7d\", spark_sum(\"total_revenue\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9ce7045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# one folder per date partitionBy('date')\n",
    "\n",
    "df_final.write.mode(\"overwrite\").partitionBy(\"date\").parquet(\"out/daily_kpi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf747d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out/daily_kpi/date=2025-10-23 | total - 48 | partitions - 1\n",
      "+-------+-----------+------------+-------------+-------------+------------+-----------------+\n",
      "|country|category   |total_events|num_purchases|total_revenue|unique_users|revenue_7d       |\n",
      "+-------+-----------+------------+-------------+-------------+------------+-----------------+\n",
      "|BR     |books      |11          |0            |0.0          |11          |66.28            |\n",
      "|BR     |electronics|9           |0            |0.0          |9           |197.13           |\n",
      "|BR     |fashion    |11          |0            |0.0          |10          |261.17           |\n",
      "|BR     |home       |6           |0            |0.0          |6           |989.4699999999999|\n",
      "|BR     |sports     |9           |0            |0.0          |9           |473.81           |\n",
      "|BR     |toys       |8           |0            |0.0          |8           |319.13           |\n",
      "|DE     |books      |13          |0            |0.0          |13          |57.47            |\n",
      "|DE     |electronics|8           |0            |0.0          |8           |290.15           |\n",
      "|DE     |fashion    |6           |1            |90.59        |6           |403.4            |\n",
      "|DE     |home       |6           |0            |0.0          |6           |322.91           |\n",
      "+-------+-----------+------------+-------------+-------------+------------+-----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_sample = spark.read.parquet(\"out/daily_kpi/date=2025-10-23\")\n",
    "\n",
    "print(\n",
    "    f\"out/daily_kpi/date=2025-10-23 | total - {df_sample.count()} | partitions - {df_sample.rdd.getNumPartitions()}\"\n",
    ")\n",
    "df_sample.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d2a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: repartition findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
